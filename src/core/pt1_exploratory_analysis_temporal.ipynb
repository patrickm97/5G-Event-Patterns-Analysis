{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: The column use_case_id was manually added to the UCs in PostgreSQL before extracting them as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of Use Case CSV files in the input directory\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "# dictionary containing the information of the UCs with over 100k rows\n",
    "# key = UC | value = n_rows\n",
    "dic_UCs = {\n",
    "  \"l001_bb\": 898147,\n",
    "  \"l017_bb\": 304192,\n",
    "  \"l020\": 116533,\n",
    "  \"l026_bb\": 238681,\n",
    "  \"l029_bb\": 7150609,\n",
    "  \"l029_du\": 860512,\n",
    "  \"l032\": 3063568,\n",
    "  \"l033\": 385771,\n",
    "  \"l040\": 111034,\n",
    "  \"l080\": 151111,\n",
    "  \"l081\": 3722470,\n",
    "  \"l083\": 21672526,\n",
    "  \"l090\" : 24583957\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following 2 cells were used only to get the number of rows of each input CSV file for the research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../input/'\n",
    "\n",
    "# list with the UC DataFrames\n",
    "uc_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    path = os.path.join(directory, file)\n",
    "    # ler apenas os nomes das colunas do CSV para determinar as colunas selecionadas\n",
    "    header_data = pd.read_csv(path, nrows=0)\n",
    "\n",
    "    # ler apenas os nomes das colunas do CSV para determinar as colunas selecionadas\n",
    "    header_data = pd.read_csv(path, nrows=0)\n",
    "\n",
    "    uc = os.path.basename(path)\n",
    "\n",
    "    # dictionary to store the indexes of the selected columns\n",
    "    dic_selected_columns = {}\n",
    "    desired_columns = ['use_case_id']\n",
    "\n",
    "    # encontrar os índices das colunas\n",
    "    for col_name in desired_columns:\n",
    "        if col_name in header_data.columns:\n",
    "            col_index = header_data.columns.get_loc(col_name)\n",
    "            dic_selected_columns[col_name] = col_index\n",
    "\n",
    "    col_indexes = list(dic_selected_columns.values())\n",
    "\n",
    "    print(f'Columns selected for UC {uc}: {dic_selected_columns}')\n",
    "    print(f'Indexes of selected columns: {col_indexes}')\n",
    "\n",
    "    uc_list.append(pd.read_csv(path, usecols=col_indexes))\n",
    "\n",
    "    print(f'Extraction of UC {uc} finished\\n')\n",
    "\n",
    "print('Extraction of all UCs finished\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in uc_list:\n",
    "    n_rows, n_cols = df.shape\n",
    "    uc = df['use_case_id'].iloc[0]\n",
    "    print(f'{uc} & {n_rows} \\\\\\\\')\n",
    "    print('\\\\hline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset exploratory analysis\n",
    "\n",
    "Convert and sort datetime columns.\n",
    "\n",
    "Join in a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../input/'\n",
    "\n",
    "# list with UCs DataFrames\n",
    "df_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    path = os.path.join(directory, file)\n",
    "\n",
    "    # get UC name from CSV file name\n",
    "    index_slash = path.rfind('/')\n",
    "    index_dot = path.rfind('.')\n",
    "    uc = ''\n",
    "    if index_slash != -1 and index_dot != -1:\n",
    "        uc = path[index_slash + 1:index_dot]\n",
    "\n",
    "    # read only the header of the CSV in order to determine the selected columns\n",
    "    header_data = pd.read_csv(path, nrows=0)\n",
    "\n",
    "    # dictionary to store the índexes of desired columns\n",
    "    dic_selected_cols = {}\n",
    "    desired_cols = ['datetime_id', 'local_datetime', 'use_case_id']\n",
    "\n",
    "    # set column indexes\n",
    "    for col_name in desired_cols:\n",
    "        if col_name in header_data.columns:\n",
    "            col_index = header_data.columns.get_loc(col_name)\n",
    "            dic_selected_cols[col_name] = col_index\n",
    "\n",
    "    # if the UC has both datetime columns with and without timezones, remove local_datetime\n",
    "    if 'datetime_id' in dic_selected_cols and 'local_datetime' in dic_selected_cols:\n",
    "        del dic_selected_cols['local_datetime']\n",
    "\n",
    "    # columns read and included in the DataFrame\n",
    "    col_indexes = list(dic_selected_cols.values())\n",
    "\n",
    "    print(f'Columns selected for UC {uc}: {dic_selected_cols}')\n",
    "    print(f'Indexes of selected columns: {col_indexes}')\n",
    "    print('Starting file read [limit of 100000 rows]...')\n",
    "\n",
    "    # if the UC has more than 100k rows (present in dic_UCs), calculate sample intervals\n",
    "    if dic_UCs.get(uc) != None:\n",
    "        # total of rows in the file (minus the header)\n",
    "        total_rows = dic_UCs[uc] - 1\n",
    "        # define the number of rows to extract to 100k\n",
    "        n_rows_to_extract = 100000\n",
    "        # sample of rows to be ignored\n",
    "        ignored_rows_sample = total_rows - n_rows_to_extract\n",
    "        # list with the numbers of the rows to be skipped chosen randomly\n",
    "        skip_rows =  random.sample(range(1, total_rows+1), ignored_rows_sample)\n",
    "        print(f'Number of rows chosen randomly to be ignored: {len(skip_rows)}')\n",
    "        df_list.append(pd.read_csv(path, skiprows=skip_rows, usecols=col_indexes))\n",
    "    # if the file has less than 100k rows, extract all of them\n",
    "    else:\n",
    "        df_list.append(pd.read_csv(path, usecols=col_indexes))\n",
    "\n",
    "    print(f'Extraction of UC {uc} finished\\n')\n",
    "\n",
    "print('Extraction of all UCs finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of datetime columns to a numeric representation\n",
    "print('Starting conversion of the datetime columns to a numeric representation...')\n",
    "\n",
    "for df in df_list:\n",
    "    uc = df['use_case_id'].iloc[0]\n",
    "    df_cols = list(df.columns)\n",
    "    \n",
    "    if 'datetime_id' in df_cols:\n",
    "        # transform datetime to numeric representation (// 10**9 to convert from nanoseconds to secondss)\n",
    "        df['datetime_id'] = pd.to_datetime(df['datetime_id'], format='%Y-%m-%d %H:%M:%S').astype(np.int64) // 10**9\n",
    "    \n",
    "    # if the DataFrame has local_datetime column (with time zone)\n",
    "    elif 'local_datetime' in df_cols:\n",
    "        # convert local_datetime to timestamp\n",
    "        df['local_datetime'] = df['local_datetime'].apply(lambda x: parser.parse(x))\n",
    "        # create datetime_utc column with + 3 hours to be able to convert to a numeric representation\n",
    "        df['datetime_utc'] = df['local_datetime'].apply(lambda x: x.tz_localize(None) + pd.Timedelta(hours=3))\n",
    "        # print first row of datetime_utc column to check if it is correct (should have the same value of the original + 3 hours)\n",
    "        print(df['datetime_utc'].iloc[0])\n",
    "        # create datetime_id column and convert to numeric representation\n",
    "        df['datetime_id'] = df['datetime_utc'].apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s'))\n",
    "        # remove local_datetime and datetime_utc columns\n",
    "        df = df.drop(['local_datetime', 'datetime_utc'], axis=1)\n",
    "\n",
    "    print(f'\\nConversion of datetime of UC {uc} finished')\n",
    "\n",
    "print(f'\\nConversion of all UCs datetime columns to a numerical representation finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set order of columns to datetime_id | use_case_id\n",
    "for df in df_list:\n",
    "    # if the columns are not in the correct order, invert them\n",
    "    if df.columns[0] == 'use_case_id' and df.columns[1] == 'datetime_id':\n",
    "        df = df.rename(columns={'use_case_id': 'temp'})\n",
    "        df = df.rename(columns={'datetime_id': 'use_case_id'})\n",
    "        df = df.rename(columns={'temp': 'datetime_id'})\n",
    "\n",
    "        df['datetime_id'], df['use_case_id'] = df['use_case_id'], df['datetime_id']\n",
    "        print('Column order inverted')\n",
    "    \n",
    "    else:\n",
    "        print('Column order already as datetime_id | use_case_id, no changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print general information of DataFrames\n",
    "for df in df_list:\n",
    "    print(df.info())\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 2 rows of each DataFrame\n",
    "for df in df_list:\n",
    "    print(df.head(2))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output temporal CSV files\n",
    "for i, df in enumerate(df_list):\n",
    "    file_name = df['use_case_id'].iloc[0]\n",
    "    df.to_csv('../output/temporal_'+ file_name, index=False)\n",
    "    print(f'\"temporal_{file_name}\" generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files in output folder\n",
    "print(os.listdir('../output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all UCs are joined into a single CSV file, it is necessary to **sort the datetime_id column in ascending order**, then the table will be compatible with statistical analysis, such as **linear correlation**.\n",
    "\n",
    "Example of desired layout:\n",
    "\n",
    "|datetime_id | use_case_id|\n",
    "\n",
    "|00181       | l006       |\n",
    "\n",
    "|00181       | l080       |\n",
    "\n",
    "|02405       | l090       |\n",
    "\n",
    "|02405       | l050       |\n",
    "\n",
    "|02405       | l084       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all UCs\n",
    "UCs_list = []\n",
    "print('Joining temporal files...')\n",
    "\n",
    "df_uc001_bb = pd.read_csv('./output/temporal_l001_bb.csv', delimiter=',')\n",
    "UCs_list.append(df_uc001_bb)\n",
    "df_uc006 = pd.read_csv('./output/temporal_l006.csv', delimiter=',')\n",
    "UCs_list.append(df_uc006)\n",
    "df_uc017_bb = pd.read_csv('./output/temporal_l017_bb.csv', delimiter=',')\n",
    "UCs_list.append(df_uc017_bb)\n",
    "df_uc017_du = pd.read_csv('./output/temporal_l017_du.csv', delimiter=',')\n",
    "UCs_list.append(df_uc017_du)\n",
    "df_uc018 = pd.read_csv('./output/temporal_l018.csv', delimiter=',')\n",
    "UCs_list.append(df_uc018)\n",
    "df_uc020 = pd.read_csv('./output/temporal_l020.csv', delimiter=',')\n",
    "UCs_list.append(df_uc020)\n",
    "df_uc026_bb = pd.read_csv('./output/temporal_l026_bb.csv', delimiter=',')\n",
    "UCs_list.append(df_uc026_bb)\n",
    "df_uc026_du = pd.read_csv('./output/temporal_l026_du.csv', delimiter=',')\n",
    "UCs_list.append(df_uc026_du)\n",
    "df_uc029_bb = pd.read_csv('./output/temporal_l029_bb.csv', delimiter=',')\n",
    "UCs_list.append(df_uc029_bb)\n",
    "df_uc029_du = pd.read_csv('./output/temporal_l029_du.csv', delimiter=',')\n",
    "UCs_list.append(df_uc029_du)\n",
    "df_uc032 = pd.read_csv('./output/temporal_l032.csv', delimiter=',')\n",
    "UCs_list.append(df_uc032)\n",
    "df_uc033 = pd.read_csv('./output/temporal_l033.csv', delimiter=',')\n",
    "UCs_list.append(df_uc033)\n",
    "df_uc037_du = pd.read_csv('./output/temporal_l037_du.csv', delimiter=',')\n",
    "UCs_list.append(df_uc037_du)\n",
    "df_uc040 = pd.read_csv('./output/temporal_l040.csv', delimiter=',')\n",
    "UCs_list.append(df_uc040)\n",
    "df_uc041 = pd.read_csv('./output/temporal_l041.csv', delimiter=',')\n",
    "UCs_list.append(df_uc041)\n",
    "df_uc044 = pd.read_csv('./output/temporal_l044.csv', delimiter=',')\n",
    "UCs_list.append(df_uc044)\n",
    "df_uc050 = pd.read_csv('./output/temporal_l050.csv', delimiter=',')\n",
    "UCs_list.append(df_uc050)\n",
    "df_uc080 = pd.read_csv('./output/temporal_l080.csv', delimiter=',')\n",
    "UCs_list.append(df_uc080)\n",
    "df_uc081 = pd.read_csv('./output/temporal_l081.csv', delimiter=',')\n",
    "UCs_list.append(df_uc081)\n",
    "df_uc083 = pd.read_csv('./output/temporal_l083.csv', delimiter=',')\n",
    "UCs_list.append(df_uc083)\n",
    "df_uc084 = pd.read_csv('./output/temporal_l084.csv', delimiter=',')\n",
    "UCs_list.append(df_uc084)\n",
    "df_uc090 = pd.read_csv('./output/temporal_l090.csv', delimiter=',')\n",
    "UCs_list.append(df_uc090)\n",
    "\n",
    "df_union = pd.concat(UCs_list)\n",
    "n_rows, n_cols = df_union.shape\n",
    "print(f'Number of UCs joined: {len(UCs_list)} | Rows: {n_rows} | Columns: {n_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort DataFrame of joined UCs:\n",
    "df_union = df_union.sort_values(by='datetime_id')\n",
    "print('datetime_id column sorted in ascending order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime_id back to timestamp format\n",
    "df_union['datetime_id'] = pd.to_datetime(df_union['datetime_id'], unit='s').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print('datetime_id converted back to format \"Y-m-d H:M:S\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of joined UCs DataFrame to a new CSV file\n",
    "df_union.to_csv('../output/temporal_UCs.csv', index=False)\n",
    "print(f'\"temporal_UCs.csv\" generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
