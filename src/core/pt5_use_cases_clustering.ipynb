{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_scolore\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV with grouped ocurrences\n",
    "\n",
    "# method can be euclidean or dtw\n",
    "method = 'euclidean'\n",
    "\n",
    "path = '../output/temporal_UCs_pivot_seg.csv'\n",
    "\n",
    "data = pd.read_csv(path, delimiter=',')\n",
    "\n",
    "n_rows, n_cols = data.shape\n",
    "file_name = os.path.basename(path)\n",
    "print(f'File reading \"{file_name}\" finished, {n_rows} rows and {n_cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove datetime_id column from DataFrame so it doesn't show up in the plots\n",
    "data = data.drop(['datetime_id'], axis=1)\n",
    "print('datetime_id column removed from DataFrame')\n",
    "\n",
    "n_rows, n_cols = data.shape\n",
    "print(f'DataFrame: {n_rows} rows and {n_cols} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "standard_deviation = np.std(data)\n",
    "\n",
    "print('Mean:')\n",
    "print(mean)\n",
    "\n",
    "print('Standard deviation:')\n",
    "print(standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize with mean = 0.0 and std = 0.0\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0)\n",
    "print('Starting data standardization...')\n",
    "data = scaler.fit_transform(data)\n",
    "print('Standardization finished\\n')\n",
    "\n",
    "mean = np.mean(data)\n",
    "standard_deviation = np.std(data)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Standard deviation: {standard_deviation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# number of components, starts with 2 and increases iteratively\n",
    "n_cols_pca = 2\n",
    "\n",
    "# transform DataFrame to format compatible with PCA and silhouette_score\n",
    "data_2d = data.reshape(data.shape[0], -1)\n",
    "\n",
    "# define least amount of components where preserved initial data >= 90%\n",
    "while True:\n",
    "    pca = PCA(n_components=n_cols_pca)\n",
    "    pca.fit(data_2d)\n",
    "    percentage_preserved_pca = sum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "    if percentage_preserved_pca >= 90.0:\n",
    "        break\n",
    "    else:\n",
    "        n_cols_pca += 1\n",
    "\n",
    "print(f'Least amount of columns to preserve >= 90% of initial dataset information: {n_cols_pca}')\n",
    "\n",
    "pca = PCA(n_components=n_cols_pca)\n",
    "print(f'\\Starting dimensionality reduction from {n_cols} to {n_cols_pca} columns with PCA...')\n",
    "pca = pca.fit(data_2d)\n",
    "data = pca.transform(data_2d)\n",
    "\n",
    "# if fit_tranform is used, it is not possible to use explained_variance_ratio_\n",
    "# data = pca.fit_transform(data_std)\n",
    "\n",
    "# percentage preserved after PCA\n",
    "percentage_preserved_pca = sum(pca.explained_variance_ratio_) * 100\n",
    "print('%5.2f%% of initial dataset information preserved after dimensionality reduction with PCA' % percentage_preserved_pca)\n",
    "\n",
    "print('\\nSample of first dataset row after PCA')\n",
    "print(data[0])\n",
    "\n",
    "# convert dataset to DataFrame\n",
    "pca_cols_names = []\n",
    "\n",
    "for i in range(0, n_cols_pca):\n",
    "    pca_cols_names.append('component' + str(i+1))\n",
    "\n",
    "data = pd.DataFrame(data, columns=pca_cols_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define number of clusters for K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method plot to define the number of clusters\n",
    "\n",
    "# list with inertia values\n",
    "inertia_list = []\n",
    "# list with silhouette values\n",
    "silhouette_list = []\n",
    "\n",
    "# uncomment to hard-code desired method to euclidean or dtw\n",
    "# method = 'euclidean'\n",
    "\n",
    "# uncomment if PCA cell was not executed\n",
    "# data_2d = data.reshape(data.shape[0], -1)\n",
    "\n",
    "# define range to test K-Means\n",
    "# range_clusters = range(2, 15)\n",
    "range_clusters = range(2, 10) # 2 to 9 (used in the research paper)\n",
    "\n",
    "print(f'Number of clusters training ({method})\\n')\n",
    "for n_clusters in range_clusters:\n",
    "    if method == 'euclidean':\n",
    "        model = TimeSeriesKMeans(n_clusters=n_clusters, metric='euclidean', max_iter=50, n_jobs=-1, random_state=0)\n",
    "    elif method == 'dtw':\n",
    "        model = TimeSeriesKMeans(n_clusters=n_clusters,\n",
    "                                 metric='dtw',\n",
    "                                 max_iter=50,\n",
    "                                 max_iter_barycenter=100,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=0\n",
    "                                )\n",
    "        \n",
    "    with tqdm(total=1, desc=f'Training K-Means | Number of clusters = {n_clusters} | Method = {method}', dynamic_ncols=True) as pbar_kmeans:\n",
    "        model.fit(data)\n",
    "        pbar_kmeans.update(1)\n",
    "    \n",
    "    inertia_list.append(model.inertia_)\n",
    "    training_labels = model.labels_\n",
    "\n",
    "    # it is possible to calculate Silhouette only if the number of clusters is > 1\n",
    "    if len(np.unique(training_labels)) > 1:\n",
    "        with tqdm(total=1, desc=f'Silhouette calculation | Number of clusters = {n_clusters}', dynamic_ncols=True) as pbar:\n",
    "            silhouette_kmeans_elbow = silhouette_score(data_2d, training_labels)\n",
    "            pbar.set_postfix({'Silhouette': silhouette_kmeans_elbow})\n",
    "            pbar.update()\n",
    "            silhouette_list.append(silhouette_kmeans_elbow)\n",
    "\n",
    "print('')\n",
    "#print(inertia)\n",
    "\n",
    "# number of clusters = max valor de Silhouette in list (list index +2 because if starts from 2 (clusters) to len -1)\n",
    "n_clusters = silhouette_list.index(max(silhouette_list)) + 2\n",
    "print(f'\\nBest number de clusters (k): {n_clusters}')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range_clusters, inertia_list, marker='o', linestyle='-')\n",
    "plt.title(f'Elbow method to choose the k number of clusters ({method})')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform values in silhouette list to a dictionary, key = k clusters (starts with 2)\n",
    "dic_silhouette = {}\n",
    "\n",
    "for i, silhouette in enumerate(silhouette_list):\n",
    "    dic_silhouette[i+2] = silhouette\n",
    "\n",
    "dic_silhouette = {key: round(value, 2) for key, value in dic_silhouette.items()}\n",
    "for key, val in dic_silhouette.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means execution for CSV output\n",
    "\n",
    "# uncomment to change method\n",
    "# method = 'dtw'\n",
    "\n",
    "# comment if Elbow method cell was executed\n",
    "n_clusters = 8\n",
    "\n",
    "# uncomment if PCA call was not executed\n",
    "# data_2d = data.reshape(data.shape[0], -1)\n",
    "\n",
    "def kmeans_fit(n_clusters, method, model, data, i):\n",
    "    with tqdm(total=1, desc=f'Training K-Means | execution {i} | Number of clusters = {n_clusters} | Method = {method}', dynamic_ncols=True) as pbar_kmeans:\n",
    "        model.fit(data)\n",
    "        pbar_kmeans.update(1)\n",
    "    labels = model.labels_\n",
    "    inertia_kmeans = model.inertia_\n",
    "    return labels, inertia_kmeans\n",
    "\n",
    "def silhouette(n_clusters, labels, data):\n",
    "    with tqdm(total=1, desc=f'Silhouette calculation | Number of clusters = {n_clusters}', dynamic_ncols=True) as pbar:\n",
    "        silhouette_kmeans = silhouette_score(data, labels)\n",
    "        pbar.set_postfix({'Silhouette': silhouette_kmeans})\n",
    "        pbar.update(1)\n",
    "    return silhouette_kmeans\n",
    "\n",
    "print(f'K-Means ({method}) with {n_clusters} clusters\\n')\n",
    "\n",
    "# run 20 times with random_state not defined, use best result\n",
    "final_labels = []\n",
    "final_inertia = 0\n",
    "final_silhouette = 0\n",
    "\n",
    "for i in range(1, 21):\n",
    "    if method == 'euclidean':\n",
    "        kmeans_euc = TimeSeriesKMeans(n_clusters=n_clusters,\n",
    "                                    metric='euclidean',\n",
    "                                    max_iter=50,\n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "        # fit model\n",
    "        labels_euc, inertia_kmeans_euc = kmeans_fit(n_clusters, method, kmeans_euc, data, i)\n",
    "        print(f'Inertia: {inertia_kmeans_euc}')\n",
    "        # calculate silhouette score\n",
    "        if len(np.unique(labels_euc)) > 1:\n",
    "            silhouette_kmeans_euc = silhouette(n_clusters, labels_euc, data)\n",
    "        # if it was the best execution so far\n",
    "        if silhouette_kmeans_euc > final_silhouette:\n",
    "            final_labels = labels_euc\n",
    "            final_silhouette = silhouette_kmeans_euc\n",
    "            final_inertia = inertia_kmeans_euc\n",
    "    \n",
    "    if method == 'dtw':\n",
    "        kmeans_dtw = TimeSeriesKMeans(n_clusters=n_clusters, \n",
    "                                    metric='dtw',\n",
    "                                    max_iter=50,\n",
    "                                    max_iter_barycenter=100,\n",
    "                                    n_jobs=-1 # -1 uses all CPU threads in paralell\n",
    "                                    )\n",
    "        # fit model\n",
    "        labels_dtw, inertia_kmeans_dtw = kmeans_fit(n_clusters, method, kmeans_dtw, data, i)\n",
    "        print(f'Inertia: {inertia_kmeans_dtw}')\n",
    "        # calculate silhouette score\n",
    "        if len(np.unique(labels_euc)) > 1:\n",
    "            silhouette_kmeans_dtw = silhouette(n_clusters, labels_dtw, data_2d)\n",
    "        # if it was the best execution so far\n",
    "        if silhouette_kmeans_dtw > final_silhouette:\n",
    "            final_labels = labels_dtw\n",
    "            final_silhouette = silhouette_kmeans_dtw\n",
    "            final_inertia = inertia_kmeans_dtw\n",
    "\n",
    "print(f'\\Best execution: Silhouette Score: {final_silhouette} | Inertia : {final_inertia}')\n",
    "\n",
    "# print cluster of each row\n",
    "# for i, label in enumerate(labels_euc):\n",
    "#     print(f\"Série {i}: Cluster {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to CSV with labels\n",
    "\n",
    "# read grouped CSV file again\n",
    "data = pd.read_csv(path, delimiter=',')\n",
    "\n",
    "file_name = os.path.basename(path)\n",
    "n_rows, n_cols = data.shape\n",
    "print(f'File reading {file_name} finished, {n_rows} rows and {n_cols} columns')\n",
    "\n",
    "# add column with cluster labels\n",
    "data['cluster'] = final_labels\n",
    "\n",
    "# output file\n",
    "data.to_csv('../kmeans/kmeans_' + method + '_' +  str(n_clusters) + '_clusters.csv', index=False)\n",
    "print('File kmeans_' + method + '_' + str(n_clusters) + '_clusters.csv created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV with clusters\n",
    "path_cluster = '../kmeans/kmeans_euclidean_8_clusters.csv'\n",
    "df_kmeans = pd.read_csv(path_cluster, delimiter=',')\n",
    "\n",
    "n_rows, n_cols = df_kmeans.shape\n",
    "file_name = os.path.basename(path_cluster)\n",
    "print(f'File reading {file_name} finished, {n_rows} rows and {n_cols} columns')\n",
    "\n",
    "# Number of clusters in file\n",
    "n_clusters = df_kmeans['cluster'].nunique()\n",
    "print(f'Number of clusters: {n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split DataFrame, with each one being of one cluster\n",
    "list_df_kmeans = []\n",
    "\n",
    "for i in range(0, n_clusters):\n",
    "    list_df_kmeans.append(df_kmeans[df_kmeans['cluster'] == i])\n",
    "\n",
    "print('List of DataFrames for each cluster created')\n",
    "\n",
    "# view statistics of each DataFrame\n",
    "# for df in list_df_kmeans:\n",
    "#     print(df.describe())\n",
    "#     print('-'*80)\n",
    "\n",
    "#list_df_kmeans[3].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter DataFrames to keep only the UCs with mean >= 1\n",
    "for i, df in enumerate(list_df_kmeans):\n",
    "    filtered_ucs = ['datetime_id'] + [column for column in df.columns[1:-1] if df[column].mean() >= 1] + ['cluster']\n",
    "    list_df_kmeans[i] = df[filtered_ucs]\n",
    "\n",
    "print('DataFrames filtered, UCs with mean >= 1 kept')\n",
    "\n",
    "# define DataFrame to view in the plots\n",
    "cluster = 3\n",
    "\n",
    "# view statistics of each cluster\n",
    "# for df in list_df_kmeans:\n",
    "#     print(df.describe())\n",
    "#     print('-'*80)\n",
    "\n",
    "# for df in list_df_kmeans:\n",
    "#     print(df.info())\n",
    "#     print('****'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define colors for the UCs in the scatter plot\n",
    "colors = sns.color_palette(\"hsv\", len(list_df_kmeans[cluster].columns[1:-1]))\n",
    "print(f'len colors: {len(colors)}')\n",
    "\n",
    "# list of UCs correspondent to colors\n",
    "columns = list_df_kmeans[cluster].columns[1:-1]\n",
    "columns = columns.to_list()\n",
    "print(f'len columns: {len(columns)}\\n')\n",
    "\n",
    "# dicitionary with the colors of each UC\n",
    "map_colors = {column: color for column, color in zip(columns, colors)}\n",
    "\n",
    "for key, value in map_colors.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add datetime_id_d column (rounded in days to use for filtering)\n",
    "list_df_kmeans[cluster]['datetime_id_d'] = pd.to_datetime(list_df_kmeans[cluster]['datetime_id']).dt.strftime('%Y-%m-%d')\n",
    "datetimes = list_df_kmeans[cluster]['datetime_id_d'].unique()\n",
    "n_datetimes = list_df_kmeans[cluster]['datetime_id_d'].nunique()\n",
    "\n",
    "print(f'{n_datetimes} datetimes in DataFrame')\n",
    "print(datetimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover the days which a certain UC ocurred the most\n",
    "uc_search = 'l017_du'\n",
    "map_ocurrences_per_day = {}\n",
    "\n",
    "for idx, row in list_df_kmeans[cluster].iterrows():\n",
    "    if row[uc_search] > 0:\n",
    "        # extract day from datetime_id and format as '%Y-%m-%d'\n",
    "        day = pd.to_datetime(row['datetime_id']).strftime('%Y-%m-%d')\n",
    "    \n",
    "        # check if the day is already in the map, if not, create an entry in the map for it\n",
    "        if day not in map_ocurrences_per_day:\n",
    "            map_ocurrences_per_day[day] = 0\n",
    "        \n",
    "        # add the ocorrence to the correspondent day in the map\n",
    "        map_ocurrences_per_day[day] += row[uc_search]\n",
    "\n",
    "for key, value in map_ocurrences_per_day.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "# chosen day = item with most ocurrences of UC searched\n",
    "day_search = max(map_ocurrences_per_day, key=map_ocurrences_per_day.get)\n",
    "ocurrences = map_ocurrences_per_day[day_search]\n",
    "print(f'\\nDatetime with most ocurrences of UC {uc_search}: {day_search} with {ocurrences} ocurrences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN WITH CELL BELOW IS EXECUTED\n",
    "# choose day to filter with the max mean of UCs ocurrences\n",
    "day = ''\n",
    "day_plus_1 = ''\n",
    "max_percentages_sum = 0\n",
    "day_filter = ''\n",
    "day_plus_1_filter = ''\n",
    "\n",
    "for i, row in enumerate(list_df_kmeans[cluster]):\n",
    "    day = datetimes[i]\n",
    "    day = pd.Timestamp(day)\n",
    "    day_plus_1 = day + timedelta(days=1)\n",
    "    day_plus_1 = day_plus_1.strftime('%Y-%m-%d')\n",
    "    day = day.strftime('%Y-%m-%d')\n",
    "    df_filtered = list_df_kmeans[cluster].query(f\"'{day}' <= datetime_id < '{day_plus_1}'\")\n",
    "    # -2 para não considerar a coluna com o label do cluster\n",
    "    positive_percentages = (df_filtered.iloc[:,1:-2] > 0).sum(axis=1) / (len(df_filtered.columns)-2) * 100\n",
    "    sum_positive_percentages = 0\n",
    "    for percentage in positive_percentages:\n",
    "        sum_positive_percentages += percentage\n",
    "        if sum_positive_percentages > max_percentages_sum:\n",
    "            max_percentages_sum = sum_positive_percentages\n",
    "            day_filter = day\n",
    "            day_plus_1_filter = day_plus_1\n",
    "\n",
    "day = day_filter\n",
    "day_plus_1 = day_plus_1_filter\n",
    "\n",
    "max_percentages_sum = round(max_percentages_sum, 2)\n",
    "print(f'\\Largest sum of UCs ocurrences percentages in {day}: {max_percentages_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN IF CELL ABOVE WAS EXECUTED\n",
    "# hard-code date range\n",
    "day = '2023-03-01'\n",
    "day = pd.Timestamp(day)\n",
    "\n",
    "day_plus_1 = day + timedelta(days=1)\n",
    "day_plus_1 = day_plus_1.strftime('%Y-%m-%d')\n",
    "\n",
    "day = day.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'Date range set between {day} and {day_plus_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of cluster 3\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# extract corresponding colors and each column using map_colors\n",
    "colors = [map_colors[col] for col in list_df_kmeans[cluster].columns[1:-2]]\n",
    "\n",
    "# filter dataset to reduce its size\n",
    "subset = list_df_kmeans[cluster].query(f\"'{day}' <= datetime_id < '{day_plus_1}'\")\n",
    "\n",
    "# if number of datetimes in day > 50, filter hours range to 00:00-11:00 OR 11:15-23:45\n",
    "if len(subset) > 50:\n",
    "    subset = subset.iloc[0:50] # first 50 values\n",
    "    #subset = subset.iloc[len(subset)-51:-1] # last 50 values\n",
    "    #subset = subset.iloc[17:68] # hard-code hours range\n",
    "##########################################################################################\n",
    "\n",
    "# number of datetimes in subset\n",
    "n_datetimes_subset = subset['datetime_id'].nunique()\n",
    "print(f'Datetimes: {n_datetimes_subset}')\n",
    "\n",
    "# first and last datetime in filtered day\n",
    "data1 = subset.iloc[0]['datetime_id']\n",
    "data2 = subset.iloc[-1]['datetime_id']\n",
    "\n",
    "## set highlighted UCs to display only their dots and lines in the plot\n",
    "uc1 = 'l017_du'\n",
    "uc2 = 'l090'\n",
    "#uc3 = 'l081'\n",
    "\n",
    "values_ucs1 = subset[uc1]\n",
    "values_ucs2 = subset[uc2]\n",
    "#values_ucs3 = subset[uc3]\n",
    "\n",
    "# set list of highlighted UCs\n",
    "list_highlighted_ucs = [uc1, uc2]\n",
    "#list_highlighted_ucs = [uc1, uc2, uc3]\n",
    "\n",
    "##################################### UCs dots ###############################################\n",
    "for i, col in enumerate(list_df_kmeans[cluster].columns[1:-2]):\n",
    "    # display only the dots of highlighted UCs\n",
    "    if col in list_highlighted_ucs:\n",
    "        plt.scatter(subset['datetime_id'], subset[col], color=colors[i], label=col, alpha=0.7)\n",
    "\n",
    "################################# selected UCs lines #########################################\n",
    "plt.plot(subset['datetime_id'], values_ucs1, color=map_colors[uc1], label=uc1, linewidth=2)\n",
    "plt.plot(subset['datetime_id'], values_ucs2, color=map_colors[uc2], label=uc2, linewidth=2)\n",
    "#plt.plot(subset['datetime_id'], values_ucs3, color=map_colors[uc3], label=uc3, linewidth=2)\n",
    "##############################################################################################\n",
    "\n",
    "############### change plot title accordingly to highlighted UCs ######## ####################\n",
    "plt.title(f'Use Cases - Cluster {cluster} on {data1[0:10]} (highlight between {uc1} and {uc2})')\n",
    "#plt.title(f'Use Cases - Cluster {cluster} on {data1[0:10]} (highlight between {uc1}, {uc2} and {uc3})')\n",
    "#plt.title(f'Use Cases - Cluster {cluster} on {data1[0:10]} (highlight between {uc1}/{uc2[-2:]} and {uc3})')\n",
    "#plt.title(f'Use Cases - Cluster {cluster} on {data1[0:10]}')\n",
    "#plt.title(f'Use Cases - Cluster {cluster} on {data1[0:10]} (highlight between {uc1} and {uc2}/{uc3[-2:]})')\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('UCs Ocurrences')\n",
    "\n",
    "# create legends using the mapping between UCs names and colors\n",
    "legend_labels = [plt.Line2D([0], [0], marker='o', color='w', label=f' {key}', markersize=10, markerfacecolor=val) for key, val in map_colors.items()]\n",
    "plt.legend(handles=legend_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# rotate x axis labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
